{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a54d83-6897-4df0-accf-21e015c64a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading Libraries!\n"
     ]
    }
   ],
   "source": [
    "# import all dependencies and assignment of variables\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert openai.api_key is not None\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "CALENDAR_URL = f\"{BASE_URL}/monetarypolicy/fomccalendars.htm\"\n",
    "START_YEAR = 2024\n",
    "OUTPUT_DIR = \"minutes_texts\"\n",
    "print('Done Loading Libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f81bee1-acbe-4c43-938a-cca2ee3c1fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading Models\n"
     ]
    }
   ],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "print('Done Loading Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055dc51-3b1b-496d-9a4c-787cfea21d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_finbert_sentiment(text, chunk_size=512):\n",
    "    #Get FinBERT sentiment scores for text\n",
    "    # Split text into chunks to handle long documents\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    sentiment_scores = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        sentiment_scores.append(scores.detach().numpy()[0])\n",
    "    avg_scores = np.mean(sentiment_scores, axis=0)\n",
    "    return {'positive': float(avg_scores[0]),'negative': float(avg_scores[1]),'neutral': float(avg_scores[2])}\n",
    "\n",
    "def scrape_and_save_minutes():\n",
    "    #Scrape FOMC minutes and save them to text files\n",
    "    try:\n",
    "        print(\"Fetching FOMC calendar page...\")\n",
    "        response = requests.get(CALENDAR_URL)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        panels = soup.find_all('div', class_='panel-default')\n",
    "        minutes_dict = {}\n",
    "        for panel in panels:\n",
    "            heading = panel.find('div', class_='panel-heading')\n",
    "            if not heading: continue\n",
    "            year_match = re.search(r'(\\d{4})', heading.get_text())\n",
    "            if not year_match: continue\n",
    "            year = int(year_match.group(1))\n",
    "            if year < START_YEAR: continue\n",
    "            print(f\"\\nProcessing year {year}...\")\n",
    "            meeting_rows = panel.find_all('div', class_='fomc-meeting')\n",
    "            print(f\"Found {len(meeting_rows)} meeting rows\")\n",
    "            for row in meeting_rows:\n",
    "                minutes_div = row.find('div', class_='fomc-meeting__minutes')\n",
    "                if not minutes_div: continue\n",
    "                links = minutes_div.find_all('a')\n",
    "                html_link = next((link for link in links if 'htm' in link.get('href', '').lower()), None)\n",
    "                if html_link and html_link.get('href'):\n",
    "                    full_url = f\"{BASE_URL}{html_link['href']}\"\n",
    "                    date = re.search(r'minutes(\\d{8})', full_url)\n",
    "                    if date:\n",
    "                        date_str = date.group(1)\n",
    "                        formatted_date = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "                        print(f\"Fetching minutes from {formatted_date}\")\n",
    "                        response = requests.get(full_url)\n",
    "                        response.raise_for_status()\n",
    "                        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                        article = soup.find('div', id='article')\n",
    "                        if article:\n",
    "                            paragraphs = article.find_all('p')\n",
    "                            text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    "                            if text:\n",
    "                                minutes_dict[formatted_date] = (full_url, text)\n",
    "                                os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "                                with open(os.path.join(OUTPUT_DIR, f\"minutes_{formatted_date}.txt\"), 'w', encoding='utf-8') as f:\n",
    "                                    f.write(text)\n",
    "                                print(f\"Saved minutes for {formatted_date} ({len(text)} characters)\")\n",
    "                                time.sleep(1)\n",
    "        print(f\"\\nAvailable dates:\")\n",
    "        for date in sorted(minutes_dict.keys()):\n",
    "            url, text = minutes_dict[date]\n",
    "            print(f\"{date}: {len(text)} characters\")\n",
    "        return minutes_dict\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def analyze_minutes_and_yields():\n",
    "    #Analyze minutes sentiment and yield changes [positive, negative, neutral]\n",
    "    hawkish_mapping = np.array([1, -0.5, 0])  # negative sentiment more hawkish\n",
    "    sentiment_results,yield_results = {},{}\n",
    "    # Calculate sentiment scores for all documents\n",
    "    for filename in os.listdir(OUTPUT_DIR):\n",
    "        if filename.startswith(\"minutes_\") and filename.endswith(\".txt\"):\n",
    "            print(f\"Reading: {filename}\")\n",
    "            date = filename[8:-4]\n",
    "            with open(os.path.join(OUTPUT_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            # Get FinBERT sentiment scores as array\n",
    "            scores = get_finbert_sentiment(text)\n",
    "            scores_array = np.array([scores['positive'], scores['negative'], scores['neutral']])\n",
    "            # Calculate hawkish score using dot product => This will give a score between -1 and 1\n",
    "            hawkish_score = np.dot(scores_array, hawkish_mapping)\n",
    "            sentiment_results[date] = hawkish_score\n",
    "    # Calculate yield changes\n",
    "    for date in sentiment_results.keys():\n",
    "        try:\n",
    "            start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "            end_date = start_date + timedelta(days=7)\n",
    "            print(f\"Fetching yields for {date}...\")\n",
    "            data = yf.download('^TNX', start=start_date, end=end_date, progress=False)\n",
    "            if not data.empty:\n",
    "                initial_yield = float(data['Close'].iloc[0])  # Convert to float\n",
    "                final_yield = float(data['Close'].iloc[-1])   # Convert to float\n",
    "                yield_results[date] = round(final_yield - initial_yield, 2)\n",
    "                print(f\"Successfully got yield change\")\n",
    "            else:\n",
    "                print(f\"No yield data available for {date}\")\n",
    "                yield_results[date] = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting yield data for {date}: {e}\")\n",
    "            yield_results[date] = None\n",
    "    df = pd.DataFrame({'Hawkish Score': sentiment_results,'10Y_Change': yield_results}).sort_index()\n",
    "    print(\"FOMC Minutes Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df)\n",
    "    # Create scatter plot\n",
    "    if not df['10Y_Change'].isna().all():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        valid_data = df.dropna()\n",
    "        if not valid_data.empty:\n",
    "            sns.scatterplot(data=valid_data, x='Hawkish Score', y='10Y_Change')\n",
    "            plt.title('Hawkish Score vs 10Y Treasury Yield Change')\n",
    "            plt.xlabel('Hawkish Score (-1 = Dove, 1 = Hawk)')\n",
    "            plt.ylabel('10Y Yield Change (bps)')\n",
    "            x = valid_data['Hawkish Score']\n",
    "            y = valid_data['10Y_Change']\n",
    "            z = np.polyfit(x, y, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(x, p(x), \"r--\", alpha=0.8)\n",
    "            correlation = valid_data['Hawkish Score'].corr(valid_data['10Y_Change'])\n",
    "            plt.annotate(f'Correlation: {correlation:.2f}', xy=(0.05, 0.95), xycoords='axes fraction',bbox=dict(facecolor='white', edgecolor='black', alpha=0.7))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    return df\n",
    "\n",
    "def query_minutes(date, question):\n",
    "    try:\n",
    "        file_path = os.path.join(OUTPUT_DIR, f\"minutes_{date}.txt\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"No minutes found for date {date}\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            minutes_text = f.read()\n",
    "        system_prompt_content = \"You are an expert in analyzing Federal Reserve communications.\"\n",
    "        user_prompt_content = f\"Below are FOMC minutes from a meeting. Please answer the question about these minutes based only on the information provided.\\n\\nMinutes:\\n{minutes_text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_content}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        return answer.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "def evaluate_sentiment_predictions(df):\n",
    "    eval_df = df.copy()\n",
    "    eval_df.dropna(subset=['Hawkish Score', '10Y_Change'], inplace=True)\n",
    "    eval_df = eval_df[(eval_df['Hawkish Score'] != 0) & (eval_df['10Y_Change'] != 0)]\n",
    "    if eval_df.empty:\n",
    "        print(\"No data available for evaluation after filtering.\")\n",
    "        return\n",
    "    eval_df['Predicted_Label'] = np.where(eval_df['Hawkish Score'] > 0, 1, 0)\n",
    "    eval_df['Actual_Label'] = np.where(eval_df['10Y_Change'] > 0, 1, 0)\n",
    "    cm_labels = [0, 1] # 0: Dovish, 1: Hawkish\n",
    "    cm = confusion_matrix(eval_df['Actual_Label'], eval_df['Predicted_Label'], labels=cm_labels)\n",
    "    accuracy = accuracy_score(eval_df['Actual_Label'], eval_df['Predicted_Label'])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',xticklabels=['Dovish (0)', 'Hawkish (1)'],yticklabels=['Dovish (0)', 'Hawkish (1)'],cbar=False)\n",
    "    plt.xlabel('Actual Label')\n",
    "    plt.ylabel('Predicted Label')\n",
    "    plt.title(f'Sentiment Prediction Confusion Matrix\\nAccuracy: {accuracy*100:.2f}%')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1badf8-79e8-4fa9-8bdf-9617edd53136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap minutes from the web:\n",
    "minutes_dict = scrape_and_save_minutes()  # Only needed first time or to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c6864-61c7-4fc7-84b0-c90a10e0a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse (minutes text retrieved before) => Dovish/Hawkish and Evaluate classification\n",
    "df=analyze_minutes_and_yields() \n",
    "evaluate_sentiment_predictions(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83969eba-c377-4499-bfbd-2aa5970942e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query specific minutes (Chatbot)\n",
    "date = \"2022-01-26\"\n",
    "question = \"Can you find the most dovish and the most hawkish satements in such text and classify it yourself as one label or the other?\"\n",
    "answer = query_minutes(date, question)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e0d25-da1b-4748-93fb-8fb02dcfd203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77052339-0b1a-4638-8633-6e35ecb12a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d89b07-3a44-4f25-ba31-a57809144a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582da7a-28cd-4db3-8c0a-dba102b41145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9cbf6-854c-41c2-bf86-a2ecff9acc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b711f-1dde-4640-a179-511340ff6318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc133c6-1ba4-4c55-ae78-6093c1598759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53212913-0c13-4f49-8755-b2726c3863c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991f03f-4427-4d8c-a2f3-d5d7320952f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
